"""
Medical Affairs AI Agent v5.4
=============================

Ein LangGraph-basierter Medical-Information-Agent zur automatisierten 
Klassifikation und Beantwortung von Anfragen im Pharmakontext.

Features:
---------
- Automatische Triage von Anfragen (Nebenwirkungsmeldung, medizinische Info, Sonstiges)
- RAG-basierte Antwortgenerierung mit Dokumenten-Grading
- Iterative Qualit√§tskontrolle durch Critique-Loop
- Fallback-Modus bei fehlenden Quelldokumenten
- Vollst√§ndiges Audit-Logging mit Timestamps

Abh√§ngigkeiten:
---------------
- LangGraph f√ºr State-Machine-Workflow
- LangChain f√ºr LLM-Integration und Retrieval
- Streamlit f√ºr Web-Interface
- Chroma f√ºr Vektordatenbank

Autor: [Dein Name]
Datum: 2025-12-19
"""

import os
import datetime
from typing import TypedDict, List

import streamlit as st
from dotenv import load_dotenv

from langgraph.graph import StateGraph, END
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.messages import HumanMessage
from langchain_core.documents import Document

# === DIAGNOSE BLOCK START ===
import google.generativeai as genai

try:
    # Key laden
    api_key = os.getenv("GOOGLE_API_KEY") or st.secrets.get("GOOGLE_API_KEY")
    genai.configure(api_key=api_key)
    
    st.sidebar.error("üîç API-DIAGNOSE L√ÑUFT...")
    
    # Modelle abfragen, die Embeddings k√∂nnen
    models = [m.name for m in genai.list_models() if 'embedContent' in m.supported_generation_methods]
    
    if not models:
        st.sidebar.error("‚ùå KEINE Embedding-Modelle gefunden! API deaktiviert?")
    else:
        st.sidebar.success(f"‚úÖ Verf√ºgbare Modelle: {models}")
except Exception as e:
    st.sidebar.error(f"‚ùå Diagnose fehlgeschlagen: {e}")
# === DIAGNOSE BLOCK ENDE ===


# ==============================================================================
# MULTI-LANGUAGE TEMPLATES
# ==============================================================================

TEMPLATES = {
    "DE": {
        "header": "vielen Dank f√ºr Ihre Nachricht.",
        "ae_intro": "Wir haben Ihre Meldung bez√ºglich einer Nebenwirkung erhalten und nehmen Ihr Anliegen √§u√üerst ernst. Ich habe den Sachverhalt pers√∂nlich an unsere Abteilung f√ºr Arzneimittelsicherheit (Pharmacovigilance) weitergeleitet.",
        "ae_transition": "Zu Ihrer inhaltlichen Frage kann ich Ihnen Folgendes mitteilen:",
        "fallback": "‚ö†Ô∏è HINWEIS: Die folgenden Informationen basieren auf allgemeinem medizinischen Wissen und sind nicht durch unsere interne Datenbank gedeckt.",
        "footer": "Mit freundlichen Gr√º√üen,\nDr. Eike Bent Preu√ü | Medical Affairs Manager",
        "salutation_fallback": "Sehr geehrte Damen und Herren,"
    },
    "EN": {
        "header": "thank you for your message.",
        "ae_intro": "We have received your report regarding a potential adverse event and take it very seriously. I have forwarded this matter to our Pharmacovigilance department for documentation and review.",
        "ae_transition": "Regarding your medical inquiry, I can provide the following information:",
        "fallback": "‚ö†Ô∏è NOTE: The following information is based on general medical knowledge and is not covered by our internal database.",
        "footer": "Sincerely,\nDr. Eike Bent Preu√ü | Medical Affairs Manager",
        "salutation_fallback": "Dear Sir or Madam,"
    }
}

# ==============================================================================
# CUSTOM CSS STYLING
# ==============================================================================

def apply_custom_css():
    """
    Wendet benutzerdefiniertes CSS f√ºr ein professionelles medizinisches Design an.
    
    Features:
    - Medizinisches Farbschema (Blau/Gr√ºn-T√∂ne)
    - Verbesserte Buttons mit Hover-Effekten
    - Optimierte Cards und Expander
    - Professionelle Typografie
    """
    st.markdown("""
        <style>
        /* Hauptcontainer */
        .main {
            background: linear-gradient(135deg, #f5f7fa 0%, #e8f0f7 100%);
        }
        
        /* Header Styling */
        h1 {
            color: #1e3a8a;
            font-family: 'Helvetica Neue', sans-serif;
            font-weight: 700;
            padding-bottom: 0.5rem;
            border-bottom: 3px solid #3b82f6;
        }
        
        h2 {
            color: #1e40af;
            font-family: 'Helvetica Neue', sans-serif;
            margin-top: 2rem;
        }
        
        h3 {
            color: #2563eb;
            font-family: 'Helvetica Neue', sans-serif;
        }
        
        /* Text Area Styling */
        .stTextArea textarea {
            border: 2px solid #cbd5e1;
            border-radius: 8px;
            padding: 1rem;
            font-size: 1rem;
            transition: border-color 0.3s ease;
        }
        
        .stTextArea textarea:focus {
            border-color: #3b82f6;
            box-shadow: 0 0 0 3px rgba(59, 130, 246, 0.1);
        }
        
        /* Button Styling */
        .stButton > button {
            background: linear-gradient(135deg, #3b82f6 0%, #2563eb 100%);
            color: white;
            border: none;
            border-radius: 8px;
            padding: 0.75rem 2rem;
            font-weight: 600;
            font-size: 1rem;
            transition: all 0.3s ease;
            box-shadow: 0 4px 6px rgba(59, 130, 246, 0.2);
        }
        
        .stButton > button:hover {
            background: linear-gradient(135deg, #2563eb 0%, #1e40af 100%);
            transform: translateY(-2px);
            box-shadow: 0 6px 12px rgba(59, 130, 246, 0.3);
        }
        
        /* Download Button */
        .stDownloadButton > button {
            background: linear-gradient(135deg, #10b981 0%, #059669 100%);
            color: white;
            border: none;
            border-radius: 8px;
            padding: 0.5rem 1.5rem;
            font-weight: 600;
            transition: all 0.3s ease;
            box-shadow: 0 4px 6px rgba(16, 185, 129, 0.2);
        }
        
        .stDownloadButton > button:hover {
            background: linear-gradient(135deg, #059669 0%, #047857 100%);
            transform: translateY(-2px);
            box-shadow: 0 6px 12px rgba(16, 185, 129, 0.3);
        }
        
        /* Info/Warning/Success Boxes */
        .stAlert {
            border-radius: 8px;
            border-left: 4px solid;
            padding: 1rem;
            margin: 1rem 0;
        }
        
        /* Expander Styling */
        .streamlit-expanderHeader {
            background: linear-gradient(135deg, #f1f5f9 0%, #e2e8f0 100%);
            border-radius: 8px;
            padding: 0.75rem;
            font-weight: 600;
            color: #1e3a8a;
            border: 1px solid #cbd5e1;
        }
        
        .streamlit-expanderHeader:hover {
            background: linear-gradient(135deg, #e2e8f0 0%, #cbd5e1 100%);
        }
        
        /* Sidebar Styling */
        [data-testid="stSidebar"] {
            background: linear-gradient(180deg, #1e3a8a 0%, #1e40af 100%);
        }
        
        [data-testid="stSidebar"] h1,
        [data-testid="stSidebar"] h2,
        [data-testid="stSidebar"] h3,
        [data-testid="stSidebar"] p {
            color: white;
        }
        
        /* Metric Cards */
        [data-testid="stMetricValue"] {
            font-size: 2rem;
            color: #1e3a8a;
            font-weight: 700;
        }
        
        /* Caption Styling f√ºr Quellen */
        .stCaption {
            background: #f8fafc;
            padding: 0.25rem 0.75rem;
            border-radius: 6px;
            border-left: 3px solid #3b82f6;
            margin: 0.25rem 0;
            font-family: 'Courier New', monospace;
        }
        
        /* Spinner Styling */
        .stSpinner > div {
            border-top-color: #3b82f6 !important;
        }
        
        /* Toast Notification */
        .stToast {
            background: linear-gradient(135deg, #10b981 0%, #059669 100%);
            color: white;
            border-radius: 8px;
        }
        
        /* Code Block Styling */
        .stMarkdown pre {
            background: #1e293b;
            color: #f8fafc; /* NEU: Helle Textfarbe f√ºr Kontrast */
            border-radius: 8px;
            padding: 1rem;
            border-left: 4px solid #3b82f6;
        }
        
        /* NEU: Sicherstellen, dass auch das innere Code-Element hell ist */
        .stMarkdown pre code {
            color: #f8fafc;
        }
        
        /* Log Text Styling */
        .stText {
            font-family: 'Courier New', monospace;
            font-size: 0.9rem;
            color: #334155;
        }
        </style>
    """, unsafe_allow_html=True)


# ==============================================================================
# 1. KONFIGURATION & SETUP
# ==============================================================================

PAGE_TITLE = "Demo: MedAffairs AI Agent v5.4"
DB_FOLDER = "chroma_db"  # Pfad zur Chroma-Vektordatenbank
REPORT_FOLDER = "reports"  # Ordner f√ºr Log-Dateien

# Standardfrage f√ºr Testzwecke
DEFAULT_QUESTION = (
    "Sehr geehrter Herr Dr. Preu√ü,\n\n"
    "Wir haben einem S√§ugling Espumisan gegeben. Kurz darauf bekam das Kind Atemnot. Sind diese Nebenwirkungen bekannt?\n\n"
    "Bitte um R√ºckmeldung.\n\n"
    "Mit freundlichen Gr√º√üen,\n"
    "Dr. Anna M√ºller"
)

# Streamlit-Konfiguration
st.set_page_config(
    page_title=PAGE_TITLE, 
    layout="wide",
    page_icon="üß¨",
    initial_sidebar_state="expanded"
)

# Custom CSS anwenden
apply_custom_css()

# Umgebungsvariablen laden (z.B. OPENAI_API_KEY)
load_dotenv()

# Verzeichnisse sicherstellen
os.makedirs(REPORT_FOLDER, exist_ok=True)

# Pr√ºfung, ob Datenbank existiert
if not os.path.exists(DB_FOLDER):
    st.error("‚ùå Datenbank nicht gefunden! Bitte erst 'indexer.py' ausf√ºhren.")
    st.stop()

# LLM-Instanz mit Gemini-Modell initialisieren
llm = ChatGoogleGenerativeAI(
    model="gemma-3-27b-it", 
    temperature=0,
    google_api_key=os.getenv("GOOGLE_API_KEY") or st.secrets.get("GOOGLE_API_KEY")
)


# Vektordatenbank und Retriever initialisieren
#embeddings = OpenAIEmbeddings()
embeddings = GoogleGenerativeAIEmbeddings(
    model="models/embedding-001",
    google_api_key=os.getenv("GOOGLE_API_KEY") or st.secrets.get("GOOGLE_API_KEY")
)
vectorstore = Chroma(persist_directory=DB_FOLDER, embedding_function=embeddings)
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})  # Top-3 Dokumente


# ==============================================================================
# 2. STATE DEFINITION
# ==============================================================================

class AgentState(TypedDict):
    """
    Zentrale State-Struktur f√ºr den LangGraph-Workflow.
    
    Attributes:
        question: Die urspr√ºngliche Nutzer-Anfrage
        category: Klassifizierte Kategorie (ADVERSE_EVENT_ONLY, HYBRID, MEDICAL_INFO, OTHER)
        context: Zusammengef√ºhrter Text aus relevanten Dokumenten
        documents: Liste der geladenen Document-Objekte
        source_names: Dateinamen der verwendeten Quellen
        draft: Generierter E-Mail-Entwurf
        critique: Feedback vom Critique-Node (PASS oder Fehlerbeschreibung)
        revision_count: Anzahl der Draft-Iterationen
        logs: Liste aller Workflow-Logs mit Timestamps
        fallback_mode: True, wenn keine DB-Dokumente gefunden wurden
        has_ae_component: True, wenn Adverse-Event-Komponente erkannt wurde
    """
    question: str
    category: str
    context: str
    documents: List[Document]
    source_names: List[str]
    draft: str
    critique: str
    revision_count: int
    logs: List[str]
    fallback_mode: bool
    has_ae_component: bool
    optimized_query: str
    language: str


def add_log(current_logs: List[str] | None, message: str) -> List[str]:
    """
    F√ºgt einen Zeitstempel-versehenen Log-Eintrag hinzu.
    
    Args:
        current_logs: Bestehende Log-Liste (kann None sein)
        message: Log-Nachricht
        
    Returns:
        Aktualisierte Log-Liste mit neuem Eintrag
    """
    timestamp = datetime.datetime.now().strftime("%H:%M:%S")
    return (current_logs or []) + [f"[{timestamp}] {message}"]


# ==============================================================================
# 3. NODE DEFINITIONS (Graph-Knoten)
# ==============================================================================

def determine_salutation(email_text: str, language: str = "DE") -> str: # <--- language hinzugef√ºgt
    """
    Extrahiert den Namen des ABSENDERS (Signatur) und erstellt eine Anrede.
    Ignoriert explizit den Namen in der Begr√º√üung (Empf√§nger).
    """
    # Sprach-Anweisung definieren
    lang_instruction = (
        "Erstelle eine deutsche Anrede (z.B. 'Sehr geehrte Frau M√ºller,')." 
        if language == "DE" 
        else "Create an English salutation (e.g. 'Dear Ms. Miller,')."
    )

    prompt = f"""
    Du bist ein Assistent, der eine Antwort auf eine eingehende E-Mail verfasst.
    Deine Aufgabe: Erstelle die Anrede f√ºr die ANTWORT-Mail an den Verfasser.

    Regeln zur Namensfindung:
    1. Suche den Namen des ABSENDERS. Dieser steht fast immer am ENDE der E-Mail (nach "Viele Gr√º√üe", "Mit freundlichen Gr√º√üen", "Signatur").
    2. WICHTIG: Ignoriere Namen, die am ANFANG der E-Mail stehen (z.B. "Hallo Dr. Preu√ü", "Guten Tag Team"). Das sind die Empf√§nger, NICHT die Absender.
    3. Beachte das Geschlecht (Dr., Herr, Frau) f√ºr die korrekte Anrede (Sehr geehrter Herr..., Sehr geehrte Frau...).
    5. Achte auf akademische Titel (Dr., Prof.) beim Absender.

    Fallback:
    - Wenn KEIN Name in der Signatur/am Ende erkennbar ist, antworte NUR mit: "Sehr geehrte Damen und Herren,"

    Sprach-Anweisung: {lang_instruction}

    E-Mail Text:
    \"\"\"
    {email_text}
    \"\"\"

    Antworte NUR mit der Anrede-Zeile ohne weitere Zeichen.
    """
    try:
        response = llm.invoke([HumanMessage(content=prompt)])
        return response.content.strip()
    except:
        return TEMPLATES[language]["salutation_fallback"]

def triage_node(state: AgentState) -> dict:
    """
    Klassifiziert die Anfrage in eine von vier Kategorien.
    """
    question = state["question"]
    # HIER WAR DER FEHLER: Am Ende fehlte die Klammer )
    logs = add_log(state.get("logs"), "TRIAGE: Analysiere Intent & Sprache...") 

    # Prompt f√ºr LLM-basierte Klassifikation
    prompt = f"""
    Du bist ein Compliance-Offizier. Kategorisiere die Anfrage in genau EINE Kategorie:
    Aufgabe 1: Bestimme die KATEGORIE der Anfrage basierend auf folgendem Schema:
    1. "ADVERSE_EVENT_ONLY": Nutzer BERICHTET NUR √ºber Vorfall/Symptome, stellt KEINE Frage.
    2. "HYBRID": Nutzer BERICHTET √ºber Vorfall UND stellt eine FRAGE dazu.
    3. "MEDICAL_INFO": Nutzer stellt allgemeine Fragen ohne konkreten Patientenbezug/Vorfall.
    4. "OTHER": Spam, reine Begr√º√üung.
    Aufgabe 2: Bestimme die SPRACHE der Anfrage (DE oder EN).

    Antworte STRENG im Format: KATEGORIE | SPRACHE
    Beispiel: MEDICAL_INFO | EN
    
    Anfrage: {question}
    """
    response = llm.invoke([HumanMessage(content=prompt)])
    content = response.content.strip()
    
    # Parsing der Antwort "KATEGORIE | SPRACHE"
    try:
        parts = content.split("|")
        category = parts[0].strip()
        language = parts[1].strip().upper()
        if language not in ["DE", "EN"]: language = "DE" # Fallback
    except:
        category = "OTHER"
        language = "DE"

    has_ae = category in ("HYBRID", "ADVERSE_EVENT_ONLY")

    return {
        "category": category,
        "language": language, 
        "has_ae_component": has_ae,
        "logs": add_log(logs, f"TRIAGE: {category} (Sprache: {language})"),
    }


def adverse_event_node(state: AgentState) -> dict:
    """
    Erstellt Standardantwort f√ºr reine Nebenwirkungsmeldungen ohne Frage.
    
    Diese Anfragen erfordern keine inhaltliche Recherche, sondern nur 
    eine Best√§tigung und Weiterleitung an Pharmacovigilance.
    
    Args:
        state: Aktueller Agent-State
        
    Returns:
        Dict mit draft (Standardtext) und aktualisiertem Log
    """
    question = state["question"]
    lang = state.get("language", "DE") # Sprache laden
    logs = add_log(state.get("logs"), "ADVERSE EVENT: Generiere Best√§tigung...")

    txt = TEMPLATES[lang]
    salutation = determine_salutation(question, lang)

    # Text zusammenbauen aus Templates
    response_text = (
        f"{salutation}\n\n"
        f"{txt['header']}\n\n"
        f"{txt['ae_intro']}\n\n" # Hier endet es, da keine Frage beantwortet wird
        f"{txt['footer']}"
    )

    return {
        "draft": response_text,
        "fallback_mode": False,
        "logs": logs,
    }

def retrieve_node(state: AgentState) -> dict:
    # 1. Variablen vorbereiten
    query = state["question"]
    logs = state.get("logs", []) or []
    search_query = query  # <--- HIER: Sicherstellen, dass die Variable immer existiert!

    # 2. Query-Optimierung (deine Logik)
    if len(query) > 30: 
        logs = add_log(logs, "RETRIEVAL: Starte Query-Optimierung...")
        system_prompt = (
            "Extrahiere die medizinischen Kernbegriffe f√ºr eine Datenbanksuche. "
            "Entferne Anrede, Gru√ü und F√ºllw√∂rter. "
            "Behalte Medikamentennamen und Symptome exakt bei."
        )
        try:
            clean_query = llm.invoke([
                HumanMessage(content=f"{system_prompt}\n\nText: {query}")
            ]).content.strip()
            search_query = clean_query 
            logs = add_log(logs, f"RETRIEVAL: Optimiert zu '{search_query}'")
        except Exception as e:
            logs = add_log(logs, f"Optimierung fehlgeschlagen: {str(e)}")

    # 3. Der eigentliche Datenbank-Abruf (mit Fehler-Diagnose)
    try:
        # Falls hier der Google-Fehler passiert, fangen wir ihn ab:
        docs = retriever.invoke(search_query) 
        logs = add_log(logs, f"RETRIEVAL: {len(docs)} Dokumente gefunden.")
        return {"documents": docs, "logs": logs, "optimized_query": search_query}
    except Exception as e:
        # Dies zeigt dir jetzt den WIRKLICHEN Grund (z.B. API Key falsch, Quote voll, etc.)
        st.error(f"üö® ECHTER FEHLER VON GOOGLE: {str(e)}")
        # Wir schalten in den Fallback-Modus, damit die App nicht komplett abst√ºrzt
        return {
            "documents": [], 
            "logs": add_log(logs, f"KRITISCHER FEHLER: {str(e)}"), 
            "fallback_mode": True,
            "optimized_query": search_query
        }


def grade_documents_node(state: AgentState) -> dict:
    question = state["question"]
    # Nutze hier auch die optimierte Query, falls die Originalfrage zu "dreckig" ist
    # Das hilft dem Grader oft, den Fokus zu behalten:
    target_query = state.get("optimized_query", question)
    
    documents = state.get("documents", [])
    logs = state.get("logs", []) or []
    filtered_docs = []

    for i, doc in enumerate(documents):
        # Prompt zwingt LLM zu einer Begr√ºndung
        prompt = f"""
        Du bist ein strenger Pr√ºfer. 
        Frage: {target_query}
        Dokument-Ausschnitt: {doc.page_content}
        
        Enth√§lt das Dokument Informationen zur Beantwortung der Frage?
        Antworte mit JSON Format: {{"reason": "kurze Begr√ºndung", "score": "JA" oder "NEIN"}}
        """
        
        try:
            response = llm.invoke([HumanMessage(content=prompt)])
            content = response.content.strip()
            
            # Einfaches Parsing (robuster als json.loads bei einfachen LLM Antworten)
            is_relevant = "JA" in content or '"score": "JA"' in content
            
            # Logge die Entscheidung f√ºr jedes Dokument
            doc_snippet = doc.page_content[:30].replace("\n", " ")
            logs = add_log(logs, f"GRADING Doc #{i+1} ({doc_snippet}...): {content}")

            if is_relevant:
                filtered_docs.append(doc)
        except Exception as e:
            logs = add_log(logs, f"GRADING Error: {e}")

    if not filtered_docs:
        fallback = True
        context_text = ""
        source_names = []
        logs = add_log(logs, "GRADING: ‚ö†Ô∏è Alle Dokumente abgelehnt -> Fallback.")
    else:
        fallback = False
        context_text = "\n\n".join(d.page_content for d in filtered_docs)
        source_names = sorted({d.metadata.get("source", "Unbekannt") for d in filtered_docs})
        logs = add_log(logs, f"GRADING: {len(filtered_docs)} Dokumente akzeptiert.")

    return {
        "documents": filtered_docs,
        "context": context_text,
        "source_names": source_names,
        "fallback_mode": fallback,
        "logs": logs,
    }


def build_instruction(critique: str | None) -> str:
    """
    Erstellt Prompt-Instruction f√ºr Draft-Node.
    
    Die Instruction definiert, dass nur der inhaltliche Body geschrieben 
    werden soll (keine Anrede/Gru√üformel). Bei vorliegender Kritik wird 
    diese in die Instruction integriert.
    
    Args:
        critique: Feedback vom Critique-Node (None oder String)
        
    Returns:
        Vollst√§ndige Instruction f√ºr das LLM
    """
    base = (
        "Formuliere NUR den inhaltlichen Antwort-Absatz (Body) auf die Frage. "
        "Schreibe KEINE Anrede ('Sehr geehrte...'), KEINE Einleitung ('Vielen Dank...') "
        "und KEINE Gru√üformel am Ende. Das √ºbernimmt das System. "
        "Konzentriere dich rein auf die medizinische/sachliche Antwort."
    )
    # Bei vorhandener Kritik: Diese zur Verbesserung hinzuf√ºgen
    if critique and critique != "PASS":
        return base + f" Kritik umsetzen: {critique}"
    return base


def draft_node(state: AgentState) -> dict:
    """
    Generiert den E-Mail-Entwurf basierend auf Kontext oder Allgemeinwissen.
    
    Workflow:
    ---------
    1. LLM generiert nur inhaltlichen Body (ohne Anrede/Gru√üformel)
    2. System f√ºgt Standardelemente hinzu:
       - Header (Anrede + Dank)
       - AE-Block (bei has_ae_component=True)
       - Fallback-Warnung (bei fallback_mode=True)
       - Body (vom LLM generiert)
       - Footer (Gru√üformel)
    
    Args:
        state: Aktueller Agent-State
        
    Returns:
        Dict mit:
        - draft: Vollst√§ndiger E-Mail-Text
        - revision_count: Inkrementierter Z√§hler
        - logs: Aktualisiertes Log
    """
    question = state["question"]
    context = state.get("context", "")
    critique = state.get("critique", "")
    fallback = state.get("fallback_mode", False)
    has_ae = state.get("has_ae_component", False)
    # Sprache laden (Default DE falls leer)
    lang = state.get("language", "DE") 
    
    logs = add_log(state.get("logs"), f"DRAFT: Erstelle Antwort ({lang})...")
    
    # Templates f√ºr die aktuelle Sprache laden
    txt = TEMPLATES[lang]

    # Instruction anpassen f√ºr LLM
    instruction = build_instruction(critique)
    lang_prompt = f"Antworte in der Sprache: {lang}. " # <--- WICHTIG f√ºr das LLM

    if fallback:
        prompt = f"""
        {instruction} {lang_prompt}
        Du bist Medical Information Manager.
        ACHTUNG: Keine internen Dokumente gefunden. Antworte basierend auf Allgemeinwissen (konservativ).
        Frage: {question}
        """
    else:
        prompt = f"""
        {instruction} {lang_prompt}
        Nutze AUSSCHLIESSLICH den Kontext.
        KONTEXT: {context}
        Frage: {question}
        """

    response = llm.invoke([HumanMessage(content=prompt)])
    body_text = response.content.strip()

    # === ZUSAMMENBAU MIT TEMPLATES ===
    
    # 1. Anrede (Sprache √ºbergeben!)
    salutation = determine_salutation(question, lang)

    header = f"{salutation}\n\n{txt['header']}\n\n"

    ae_block = ""
    if has_ae:
        ae_block = f"{txt['ae_intro']}\n\n{txt['ae_transition']}\n"

    fallback_block = ""
    if fallback:
        fallback_block = f"{txt['fallback']}\n\n"

    footer = f"\n\n{txt['footer']}"

    final_response = header + ae_block + fallback_block + body_text + footer

    return {
        "draft": final_response,
        "revision_count": state.get("revision_count", 0) + 1,
        "logs": logs,
    }


def critique_node(state: AgentState) -> dict:
    """
    Pr√ºft die Qualit√§t des generierten Entwurfs.
    
    Kriterien unterscheiden sich je nach Modus:
    - Fallback: Pr√ºfung auf Vollst√§ndigkeit und Fl√ºssigkeit
    - Normal: Pr√ºfung auf Belegbarkeit durch Kontext und Halluzinationen
    
    Args:
        state: Aktueller Agent-State
        
    Returns:
        Dict mit:
        - critique: "PASS" oder detaillierte Fehlerbeschreibung
        - logs: Aktualisiertes Log
    """
    draft = state["draft"]
    question = state["question"]
    fallback = state.get("fallback_mode", False)
    logs = state.get("logs", [])

    # Kriterien je nach Modus
    criteria = (
        "1. Wurde die Frage beantwortet? 2. Klingt der Text fl√ºssig?"
        if fallback
        else "1. Sind alle Aussagen durch den Kontext belegt? 2. Keine Halluzinationen? 3. Werden Nebenwirkungen korrekt an PV verwiesen?"
    )

    # Prompt fordert nun Begr√ºndung UND Urteil
    prompt = f"""
    Du bist ein Senior Medical Reviewer. Pr√ºfe den E-Mail Entwurf streng.
    
    Frage: {question}
    Entwurf: {draft}
    Kriterien: {criteria}
    
    Antworte exakt in diesem Format:
    REASONING: [Hier deine detaillierte Begr√ºndung, warum gut oder schlecht]
    VERDICT: [PASS oder FAIL: Fehlerbeschreibung]
    """
    
    response = llm.invoke([HumanMessage(content=prompt)])
    content = response.content.strip()

    # Wir parsen die Antwort (einfaches String-Splitting)
    reasoning = "Keine Begr√ºndung generiert."
    verdict = "FAIL"

    if "VERDICT:" in content:
        parts = content.split("VERDICT:")
        reasoning = parts[0].replace("REASONING:", "").strip()
        verdict = parts[1].strip()
    else:
        verdict = content # Fallback, falls Format nicht eingehalten wird

    # 1. Logge die detaillierte Begr√ºndung (f√ºr den File-Download)
    logs = add_log(logs, f"CRITIQUE DETAIL: {reasoning}")
    
    # 2. Logge das kurze Ergebnis (f√ºr den Workflow)
    if "PASS" in verdict:
        logs = add_log(logs, "CRITIQUE RESULT: ‚úÖ PASS")
        final_critique = "PASS"
    else:
        logs = add_log(logs, f"CRITIQUE RESULT: ‚ùå {verdict}")
        final_critique = verdict

    return {"critique": final_critique, "logs": logs}


# ==============================================================================
# 4. GRAPH CONSTRUCTION (LangGraph State Machine)
# ==============================================================================

# Graph initialisieren
workflow = StateGraph(AgentState)

# Alle Nodes registrieren
workflow.add_node("triage", triage_node)
workflow.add_node("adverse_event", adverse_event_node)
workflow.add_node("retrieve", retrieve_node)
workflow.add_node("grade_documents", grade_documents_node)
workflow.add_node("draft", draft_node)
workflow.add_node("critique", critique_node)

# Einstiegspunkt definieren
workflow.set_entry_point("triage")


def check_triage(state: AgentState) -> str:
    """
    Routing-Funktion nach Triage-Node.
    
    Entscheidet basierend auf Kategorie, wohin der Flow weitergeht:
    - ADVERSE_EVENT_ONLY ‚Üí adverse_event (Standardantwort)
    - HYBRID/MEDICAL_INFO ‚Üí retrieve (RAG-Pipeline)
    - OTHER ‚Üí end (Abbruch)
    
    Args:
        state: Aktueller Agent-State
        
    Returns:
        String mit Routing-Ziel ("go_ae_only", "go_retrieve", "end")
    """
    cat = state.get("category", "OTHER")
    if cat == "ADVERSE_EVENT_ONLY":
        return "go_ae_only"
    if cat in ("HYBRID", "MEDICAL_INFO"):
        return "go_retrieve"
    if cat == "OTHER":
        return "end"
    return "go_retrieve"  # Fallback


# Conditional Edge nach Triage
workflow.add_conditional_edges(
    "triage",
    check_triage,
    {"end": END, "go_ae_only": "adverse_event", "go_retrieve": "retrieve"},
)

# Lineare Edges f√ºr Hauptpfade
workflow.add_edge("adverse_event", END)  # AE-Only ‚Üí direkt Ende
workflow.add_edge("retrieve", "grade_documents")  # Retrieve ‚Üí Grading
workflow.add_edge("grade_documents", "draft")  # Grading ‚Üí Draft
workflow.add_edge("draft", "critique")  # Draft ‚Üí Critique


def check_critique(state: AgentState) -> str:
    """
    Routing-Funktion nach Critique-Node.
    
    Entscheidet, ob weitere Revision n√∂tig ist oder Workflow beendet wird:
    - PASS oder max. 2 Revisionen erreicht ‚Üí end
    - Sonst ‚Üí retry (zur√ºck zu Draft)
    
    Args:
        state: Aktueller Agent-State
        
    Returns:
        String mit Routing-Ziel ("end", "retry")
    """
    if state.get("revision_count", 0) > 2 or state.get("critique") == "PASS":
        return "end"
    return "retry"


# Conditional Edge nach Critique (mit Retry-Loop)
workflow.add_conditional_edges(
    "critique",
    check_critique,
    {"retry": "draft", "end": END},
)

# Graph kompilieren (erstellt ausf√ºhrbare State Machine)
app = workflow.compile()


# ==============================================================================
# 5. STREAMLIT FRONTEND
# ==============================================================================

# Sidebar: Status und Visualisierung
with st.sidebar:
    st.header("‚öôÔ∏è System-Status")
    
    # Status-Metriken in sch√∂nen Karten
    col1, col2 = st.columns(2)
    with col1:
        st.metric("üóÑÔ∏è Datenbank", "Aktiv", delta="OK")
    with col2:
        st.metric("ü§ñ LLM", "gemma-3-27b-it", delta="Online")
    
    st.info(f"üìÇ DB-Pfad: `{DB_FOLDER}`")
    
    st.markdown("---")
    st.subheader("üìä Workflow-Struktur")
    
    # === HIER BEGINNT DER NEUE TEIL ===
    
    # Graphviz Visualisierung
    workflow_graph = """
    digraph {
        // Layout-Einstellungen
        rankdir=TB;
        node [shape=box, style="filled,rounded", fontname="Arial", fontsize=10, margin=0.2];
        edge [fontsize=9, fontname="Arial", color="#64748b"];

        // --- KNOTEN DEFINITIONEN ---
        START [shape=circle, label="Start", fillcolor="#e2e8f0", width=0.8];
        END [shape=doublecircle, label="Ende", fillcolor="#e2e8f0", width=0.8];

        // Entscheidungsknoten (Raute)
        TRIAGE [shape=diamond, label="Triage\n(Intent)", fillcolor="#bfdbfe", color="#1e3a8a"];
        CRITIQUE [shape=diamond, label="Critique\n(Qualit√§t)", fillcolor="#fef08a", color="#854d0e"];
        GRADE [shape=diamond, label="Grading\n(Relevanz)", fillcolor="#dcfce7", color="#166534"];

        // Aktionsknoten (Box)
        AE_NODE [label="üöë Adverse Event\n(Meldung an PV)", fillcolor="#fee2e2", color="#991b1b"];
        RETRIEVE [label="üîç Retrieval\n(DB Suche)", fillcolor="#f1f5f9", color="#475569"];
        DRAFT [label="‚úçÔ∏è Draft\n(Antwort)", fillcolor="#dbeafe", color="#1e40af"];

        // --- KANTEN / LOGIK ---
        START -> TRIAGE;
        
        TRIAGE -> AE_NODE [label="Nebenwirkung", color="#ef4444", fontcolor="#ef4444", penwidth=2];
        TRIAGE -> RETRIEVE [label="Med. Info", color="#22c55e", fontcolor="#15803d", penwidth=2];
        TRIAGE -> END [label="Spam/Other", style="dashed"];

        AE_NODE -> END;

        RETRIEVE -> GRADE;
        GRADE -> DRAFT [label="Docs OK"];
        GRADE -> DRAFT [label="Fallback", style="dashed", fontcolor="#d97706"];

        DRAFT -> CRITIQUE;
        CRITIQUE -> END [label="‚úÖ PASS", color="#22c55e", fontcolor="#15803d"];
        CRITIQUE -> DRAFT [label="‚ùå RETRY\n(max 2)", color="#eab308", fontcolor="#a16207", style="dashed"];
    }
    """
    
    # Rendern des Graphen
    with st.expander("üîç Workflow ansehen", expanded=False):
        st.graphviz_chart(workflow_graph, use_container_width=True)
    
    st.caption("üîµ Triage | üî¥ PV-Meldung | üü¢ RAG-Prozess | üü° Quality-Check")
    
    # === HIER ENDET DER NEUE TEIL ===

    st.markdown("---")
    st.markdown("### üìñ Kategorien")
    st.markdown("""
    <div style='color: white;'>
    
    - <b>ADVERSE_EVENT</b>: Nebenwirkungsmeldung
    - <b>HYBRID</b>: Meldung + Frage
    - <b>MEDICAL_INFO</b>: Allgemeine Frage
    - <b>OTHER</b>: Sonstige Anfrage
    
    </div>
    """, unsafe_allow_html=True)

# Hauptbereich: Titel und Beschreibung
st.title("üß¨ Medical Affairs AI Agent")

st.markdown("""
<div style='background: linear-gradient(135deg, #dbeafe 0%, #bfdbfe 100%); 
            padding: 1.5rem; border-radius: 10px; border-left: 5px solid #3b82f6; margin-bottom: 2rem;'>
    <h3 style='color: #1e3a8a; margin: 0;'>Intelligente Anfragenbearbeitung mit RAG & Compliance</h4>
    <p style='color: #1e40af; margin: 0.5rem 0 0 0;'>
            <strong>Features:</strong> Triage ‚Ä¢ RAG ‚Ä¢ AE-Warning ‚Ä¢ Fallback ‚Ä¢ Quality-check ‚Ä¢ Human-in-the-loop ‚Ä¢ Audit-Logging ‚Ä¢ Log-Download
    </p>
</div>
""", unsafe_allow_html=True)

# Import (falls noch nicht oben im Skript vorhanden)
import os 

# 1. Wir ermitteln den Ordner, in dem dein Skript (app_v5.4.py) liegt
script_directory = os.path.dirname(os.path.abspath(__file__))

# 2. Wir bauen den vollen Pfad zum Bild
image_path = os.path.join(script_directory, "Der KI-Assistent f√ºr Medical Affairs_Infografik.png")

# 3. Einf√ºgen in die App mit Sicherheits-Check
with st.expander("‚ÑπÔ∏è Funktionsweise: Prozess-Grafik anzeigen", expanded=False):
    if os.path.exists(image_path):
        st.image(image_path, 
                 caption="Der Workflow des Agenten im Detail", 
                 use_container_width=True)
    else:
        # Falls es immer noch nicht geht, zeigt uns diese Fehlermeldung, wo Python gesucht hat
        st.error(f"Bild nicht gefunden! Das Skript sucht hier: {image_path}")
# -------------------------


# Eingabebereich mit verbessertem Layout
st.subheader("üìù Ihre Anfrage")
email_input = st.text_area(
    "Geben Sie hier die Anfrage ein:", 
    height=150, 
    value=DEFAULT_QUESTION,
    placeholder="Beschreiben Sie Ihre medizinische Anfrage oder Nebenwirkungsmeldung..."
)

# Button zum Starten des Workflows mit verbessertem Spacing
st.markdown("<br>", unsafe_allow_html=True)
col1, col2, col3 = st.columns([2, 1, 2])
with col2:
    submit_button = st.button("üöÄ Anfrage senden", use_container_width=True, type="primary")

if submit_button:
    # Initial State mit allen Pflichtfeldern vorbelegen
    initial_state: AgentState = {
        "question": email_input,
        "revision_count": 0,
        "logs": [],
        "fallback_mode": False,
        "has_ae_component": False,
        "category": "",
        "context": "",
        "documents": [],
        "source_names": [],
        "draft": "",
        "critique": "",
    }
    
    # Workflow ausf√ºhren mit Spinner-Anzeige
    with st.spinner("üîÑ Agent analysiert und formuliert Antwort..."):
        result = app.invoke(initial_state)
        st.session_state["result"] = result

# ... nach st.session_state["result"] = result ...

if "result" in st.session_state:
    res = st.session_state["result"]
    
    # === NEUER DEBUG BEREICH ===
    st.markdown("---")
    st.subheader("üïµÔ∏è‚Äç‚ôÇÔ∏è Debugging Dashboard")
    
    # 1. Query Vergleich
    d_col1, d_col2 = st.columns(2)
    with d_col1:
        st.markdown("**Original Anfrage:**")
        st.info(res.get("question", ""))
    with d_col2:
        st.markdown("**Optimierte DB-Query:**")
        # Rot markieren, wenn sie leer oder seltsam ist
        opt_q = res.get("optimized_query", "N/A")
        if not opt_q or len(opt_q) < 3:
            st.error(f"‚ö†Ô∏è Warnung: '{opt_q}'")
        else:
            st.success(f"'{opt_q}'")

# 2. Grading Entscheidungen visualisieren
    with st.expander("üîç Detaillierte Filter-Protokolle (Grading)", expanded=True):
        # Wir filtern die Logs nach Grading-Eintr√§gen
        grading_logs = [l for l in res.get("logs", []) if "GRADING" in l]
        
        for log in grading_logs:
            # Bereinigung: Entferne Markdown-Code-Zeichen aus dem Log-Text f√ºr die Anzeige
            clean_log = log.replace("```json", "").replace("```", "").replace("{", "(\n  ").replace("}", "\n)").strip()
            
            if "Doc #" in log:
                # Entscheidung visualisieren
                if '"score": "JA"' in log or "JA" in log:
                    # HIER GE√ÑNDERT: Keine Backticks um {clean_log}
                    st.success(f"‚úÖ **AKZEPTIERT:**\n{clean_log}") 
                else:
                    # HIER GE√ÑNDERT: Keine Backticks um {clean_log}
                    st.error(f"‚ùå **ABGELEHNT:**\n{clean_log}")
            else:
                st.caption(log)

# Trennlinie
st.markdown("---")

# Ergebnis-Anzeige (nur wenn vorhanden)
if "result" in st.session_state:
    res: AgentState = st.session_state["result"]
    cat = res.get("category", "OTHER")
    
    # --- FEHLERBEHEBUNG START: Variablen definieren ---
    draft_text = res.get("draft", "")
    context_text = res.get("context", "")
    # --- FEHLERBEHEBUNG ENDE ---

    # === LOG-DATEI STRUKTURIEREN ===
    
    # 1. Metadaten Header
    log_header = f"""
==============================================================================
MEDICAL AFFAIRS AI AGENT - AUDIT LOG
==============================================================================
Date:       {datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
Category:   {cat}
Result:     {'‚úÖ Sent' if draft_text else '‚ö†Ô∏è No Draft'} 
Revision:   {res.get("revision_count", 0)}
Fallback:   {res.get("fallback_mode")}
------------------------------------------------------------------------------
Original Question:
{res.get("question")}
------------------------------------------------------------------------------
"""

    # 2. System Logs
    formatted_logs = "\n".join(res.get("logs", []))
    log_section = f"\n\n=== SYSTEM LOGS & DECISIONS ===\n{formatted_logs}"

    # 3. Finaler Entwurf
    draft_section = f"\n\n=== FINAL GENERATED DRAFT ===\n{draft_text if draft_text else 'No draft generated.'}"

    # 4. Kontext Audit
    # Falls wir Source-Namen haben, listen wir diese auch auf
    sources_list = "\n- ".join(res.get("source_names", []))
    source_section = f"\nSources used:\n- {sources_list}\n" if res.get("source_names") else ""

    audit_section = f"""
\n\n==============================================================================
APPENDIX: CONTEXT AUDIT (RAG DATA)
==============================================================================
{source_section}
USED TEXT SNIPPETS (Fed to LLM):
--------------------------------
{context_text if context_text else 'No context used (Fallback or no docs found).'}
==============================================================================
"""

    # Alles zusammenf√ºgen
    full_log_text = log_header + log_section + draft_section + audit_section

    # Timestamp f√ºr Dateinamen
    tstamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    log_path = os.path.join(REPORT_FOLDER, f"log_{tstamp}.txt")
    
    # Datei speichern
    with open(log_path, "w", encoding="utf-8") as f:
        f.write(full_log_text)

    # Header mit Kategorie-Badge und Download-Button
    col1, col2, col3 = st.columns([2, 1, 1])
    with col1:
        st.subheader("üìä Ergebnisse")
    with col3:
        st.download_button(
            label="üì• Log herunterladen",
            data=full_log_text,
            file_name=os.path.basename(log_path),
            mime="text/plain",
            use_container_width=True
        )

    # Expander f√ºr detailliertes System-Log
    with st.expander("üìú Detailliertes System-Log", expanded=False):
        st.markdown("**Workflow-Verlauf mit Timestamps:**")
        for line in res.get("logs", []):
            st.text(line)

    # === ERGEBNIS-DARSTELLUNG ===
    if draft_text:  # Jetzt funktioniert diese Abfrage, da draft_text definiert ist
        # Warnungen je nach Kategorie
        if cat == "ADVERSE_EVENT_ONLY":
            st.warning("‚ö†Ô∏è **REINE NEBENWIRKUNGSMELDUNG** ‚Äì Standardprozess aktiviert")
        elif cat == "HYBRID":
            st.warning("‚ö†Ô∏è **HYBRID-ANFRAGE** ‚Äì Nebenwirkung gemeldet + inhaltliche Frage beantwortet")

        # Zwei-Spalten-Layout
        col1, col2 = st.columns([2, 1])
        
        with col2:
            st.markdown("### üìã Metadaten")
            
            # Fallback-Status oder Quellenangaben
            if res.get("fallback_mode"):
                st.info("‚ÑπÔ∏è **Fallback-Modus**\n\nKeine DB-Dokumente gefunden")
            elif cat != "ADVERSE_EVENT_ONLY":
                st.success("‚úÖ **Quellen genutzt**")
                st.markdown("**Verwendete Dokumente:**")
                # Quelldateien auflisten
                for idx, s in enumerate(res.get("source_names", []), 1):
                    st.caption(f"{idx}. üìÑ {os.path.basename(s)}")

                # Audit-Expander
                with st.expander("üîç Kontext-Audit", expanded=False):
                    st.info("**Verwendete Textausz√ºge:**")
                    # Hier nutzen wir nun die korrekte Variable context_text
                    st.markdown(f"```text\n{context_text}\n```")
            
            # Statistiken
            st.markdown("---")
            st.metric("üîÑ Revisionen", res.get("revision_count", 0))
            st.metric("üìÑ Dokumente", len(res.get("source_names", [])))
        
        with col1:
            # E-Mail-Vorschau
            st.markdown("### ‚úâÔ∏è Generierter Antwort-Entwurf")
            st.text_area(
                "E-Mail Vorschau", 
                draft_text, 
                height=500,
                label_visibility="collapsed"
            )

            # Sende-Button
            st.markdown("<br>", unsafe_allow_html=True)
            btn_col1, btn_col2 = st.columns([1, 1])
            with btn_col1:
                if st.button("‚úâÔ∏è Antwort senden", type="primary", use_container_width=True):
                    st.toast("‚úÖ E-Mail erfolgreich versendet!", icon="‚úÖ")
                    st.success(f"‚úÖ Antwort f√ºr Ticket **#{tstamp}** wurde verschickt.")
                    st.balloons()
            with btn_col2:
                if st.button("üìã In Zwischenablage kopieren", use_container_width=True):
                    st.toast("üìã Text in Zwischenablage kopiert!", icon="üìã")
    else:
        # Kein Draft generiert
        st.info(f"‚ÑπÔ∏è **Kategorie:** {cat} ‚Äì Keine Antwort generiert.")

# Footer
st.markdown("---")
st.markdown("""
<div style='text-align: center; color: #64748b; padding: 2rem 0;'>
    <p style='margin: 0;'>üß¨ Medical Affairs AI Agent v5.4 | Powered by LangGraph & Gemma-3-27b-it</p>
    <p style='margin: 0.5rem 0 0 0; font-size: 0.9rem;'>
        ¬© 2025 | Dr. Eike Bent Preu√ü | Medical Affairs Solutions GmbH
    </p>
</div>
""", unsafe_allow_html=True)









